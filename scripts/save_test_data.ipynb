{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasank/miniconda3/envs/jtml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "\n",
    "import time\n",
    "import nvtx\n",
    "\n",
    "from loss import res_kp_loss\n",
    "from utility import *\n",
    "\n",
    "class KeypointNetModule(pl.LightningModule):\n",
    "    def __init__(self, config, wandb_run, learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(\"learning_rate\")\n",
    "        self.config = config    \n",
    "        #print(\"Pose ResNet is on device \" + str(next(self.pose_hrnet.parameters()).get_device()))     # testing line\n",
    "        #print(\"Is Pose ResNet on GPU? \" + str(next(self.pose_hrnet.parameters()).is_cuda))            # testing line\n",
    "        self.wandb_run = wandb_run\n",
    "        self.loss_fn = res_kp_loss(\n",
    "            gaussian_amp=self.config.dataset['GAUSSIAN_AMP'],\n",
    "            gaussian_sigma=self.config.dataset['GAUSSIAN_STDDEV']\n",
    "            )\n",
    "\n",
    "        # ! Good grief what a hack. This is only for local testing.\n",
    "        #WRITE_CSV = '/home/sasank/Documents/GitRepos/PnP-Solver/kp_estimates/naive_Ten_Dogs_64KP_estimates_pr.csv'\n",
    "        #self.csv_file = pd.read_csv(WRITE_CSV)\n",
    "        \n",
    "        self.in_channels = self.config.dataset['IMG_CHANNELS']\n",
    "        self.num_keypoints = self.config.dataset['NUM_KEY_POINTS']\n",
    "        self.batch_size = self.config.datamodule['BATCH_SIZE']      # This is used in the logging steps in validation_step and test_step.\n",
    "        self.image_height = self.config.dataset['IMAGE_HEIGHT']     # These two, too.\n",
    "        self.image_width = self.config.dataset['IMAGE_WIDTH']\n",
    "\n",
    "        self.my_dict = nn.ModuleDict({})\n",
    "\n",
    "        self.my_dict[\"pre_block\"] = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 3, kernel_size=1, stride=1, padding=0, bias=False),     # This is just to convert the input channels to 3, which is what the resnet expects.\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #self.my_dict[\"resnet\"] = make_resnet(3, 64, 3, 1)\n",
    "        # this \n",
    "        #self.my_dict[\"resnet\"] = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "        # ! Trying out Resnet152 to see if it improves model output\n",
    "        self.my_dict[\"resnet\"] = torchvision.models.resnet152(weights='IMAGENET1K_V2')\n",
    "\n",
    "        #assert self.num_keypoints <= 50, \"If num_keypoints > 50, the last linear layer is growing bigger, which seems unreasonable.\"\n",
    "        # The above assertion does not need to be made because it could be that the model is just finding a lower-dimensional representation of the keypoints, which, if accurate, would be a good thing.\n",
    "        # ! If model performance is bad is bad when using 64 keypoints, then it may be a good idea to reexamine this.\n",
    "\n",
    "        self.my_dict[\"keypoints\"] = nn.Sequential(\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 2 * self.num_keypoints)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"This performs a forward pass on the dataset.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): This is a tensor containing the input data.\n",
    "\n",
    "        Returns:\n",
    "            the forward pass of the dataset.\n",
    "        \"\"\"\n",
    "        x = self.my_dict[\"pre_block\"](x)\n",
    "        x = self.my_dict[\"resnet\"](x)\n",
    "        x = self.my_dict[\"keypoints\"](x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.Adam(self.parameters, lr=1e-3)\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @nvtx.annotate(\"Training step\", color=\"red\", domain=\"my_domain\")\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        training_batch, training_batch_labels = train_batch['image'], train_batch['kp_label']\n",
    "        x = training_batch\n",
    "        training_output = self(x)\n",
    "        loss = self.loss_fn(training_output, training_batch_labels)\n",
    "        #self.wandb_run.log('train/loss', loss, on_step=True)\n",
    "        #print(\"Training loss: \" + str(loss.item()))        # This print statement messes ups the pytorch_lightning progress bar lol \n",
    "        self.wandb_run.log({'train/loss': loss.item()})\n",
    "        #print(\"First outputs and labels \" + str(training_output[0]) + \" \" + str(training_batch_labels[0].item()))\n",
    "        #self.log(name=\"train/loss\", value=loss)\n",
    "        return loss\n",
    "\n",
    "    @nvtx.annotate(\"Validation step\", color=\"green\", domain=\"my_domain\")\n",
    "    def validation_step(self, validation_batch, batch_idx):\n",
    "        val_batch, val_batch_labels = validation_batch['image'], validation_batch['kp_label']\n",
    "        full_val_batch = validation_batch['full_image']     # 'Full' here means the images without subset_pixel applied\n",
    "        img_names = validation_batch['img_name']\n",
    "        x = val_batch\n",
    "        val_output = self(x)\n",
    "        loss = self.loss_fn(val_output, val_batch_labels)\n",
    "        #print(\"Validation loss: \" + str(loss.item()))      # This print statement messes ups the pytorch_lightning progress bar lol\n",
    "        self.wandb_run.log({'validation/loss': loss.item()})\n",
    "\n",
    "\n",
    "        # * Logging the predictions\n",
    "        # Must remember that val_output is a tensor of shape (batch_size, 2 * num_keypoints)\n",
    "        # And x is a tensor of shape (batch_size, 1, self.image_height, self.image_width)\n",
    "        \"\"\"\n",
    "        fig_output = plot_val_images(images=full_val_batch, preds=val_output, labels=val_batch_labels, img_names=img_names, num_keypoints=self.num_keypoints, title='Unsubsetted Image')\n",
    "        self.wandb_run.log({f'validation/val_output_{batch_idx}': fig_output})\n",
    "        # Plot the model output from what the model actually sees\n",
    "        fig_subsetted_output = plot_val_images(images=val_batch, preds=val_output, labels=val_batch_labels, img_names=img_names, num_keypoints=self.num_keypoints, title='Model View')\n",
    "        self.wandb_run.log({f'validation/epoch_{str(self.current_epoch)}_val_subsetted_output_{batch_idx}': fig_subsetted_output})\n",
    "        # Just plot the input images\n",
    "        fig_input = plot_inputs(images=full_val_batch, img_names=img_names, title='Input Image')\n",
    "        self.wandb_run.log({f'validation/epoch_{str(self.current_epoch)}_val_input_{batch_idx}': fig_input})\n",
    "        \"\"\"\n",
    "\n",
    "        # Use plot_test_images to plot the images\n",
    "        # ! Disabling val plotting for now to see how the models do\n",
    "        \"\"\"\n",
    "        fig_output_vector = plot_outputs(images=full_val_batch, preds=val_output, labels=val_batch_labels, img_names=img_names, num_keypoints=self.num_keypoints, title='Unsubsetted Image')\n",
    "        fig_subsetted_output_vector = plot_outputs(images=val_batch, preds=val_output, labels=val_batch_labels, img_names=img_names, num_keypoints=self.num_keypoints, title='Model View')\n",
    "        fig_intput_vector = plot_inputs(images=full_val_batch, img_names=img_names, title='Input Image')\n",
    "\n",
    "        for i in range(len(fig_output_vector)):\n",
    "            self.wandb_run.log({f'validation/{img_names[i]}/E{self.current_epoch}_full_output': fig_output_vector[i]})\n",
    "            self.wandb_run.log({f'validation/{img_names[i]}/E{self.current_epoch}_subsetted_output': fig_subsetted_output_vector[i]})\n",
    "            self.wandb_run.log({f'validation/{img_names[i]}/E{self.current_epoch}_input': fig_intput_vector[i]})\n",
    "        \"\"\"\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    @nvtx.annotate(\"Test step\", color=\"blue\", domain=\"my_domain\")\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        input_test_batch, test_batch_labels = test_batch['image'], test_batch['kp_label']          # 'input' here means the images inputted to the model, often with subset_pixel applied\n",
    "        full_test_batch = test_batch['full_image']     # 'Full' here means the images without subset_pixel applied\n",
    "        img_names = test_batch['img_name']\n",
    "        x = input_test_batch\n",
    "        test_output = self(x)\n",
    "        loss = self.loss_fn(test_output, test_batch_labels)\n",
    "\n",
    "        \"\"\"\n",
    "        for i in range(len(test_output)):\n",
    "            # We will plot the outputs, which are 2D keypoints, to self.csv_file in the row that corresponds to the image name\n",
    "            keypoints = test_output[i].detach().cpu().numpy()\n",
    "            # Make the keypoints a 2D array of shape (num_keypoints, 2)\n",
    "            keypoints = keypoints.reshape(self.num_keypoints, 2)\n",
    "            #keypoints.view(64,2)\n",
    "            img_name = img_names[i]\n",
    "\n",
    "            # Find the row that corresponds to the image name\n",
    "            #self.csv_file.loc[self.csv_file['Image address'] == img_name]['Femur PR KP points'] = str(keypoints)\n",
    "            self.csv_file.loc[self.csv_file['Image address'] == img_name, 'Femur PR KP points'] = str(keypoints)\n",
    "        \"\"\"\n",
    "\n",
    "        # Logging the predictions\n",
    "        # Must remember that test_output is a tensor of shape (batch_size, 2 * num_keypoints)\n",
    "        # And x is a tensor of shape (batch_size, 1, self.image_height, self.image_width)\n",
    "        data_set_name = 'naive_set' if self.config.datamodule['USE_NAIVE_TEST_SET'] else 'test_set'\n",
    "        fig_output_vector = plot_outputs(images=full_test_batch, preds=test_output, labels=test_batch_labels, img_names=img_names, num_keypoints=self.num_keypoints, title='Unsubsetted Image')\n",
    "        fig_subsetted_output_vector = plot_outputs(images=input_test_batch, preds=test_output, labels=test_batch_labels, img_names=img_names, num_keypoints=self.num_keypoints, title='Model View')\n",
    "        fig_input_vector = plot_inputs(images=full_test_batch, img_names=img_names, title='Input Image')\n",
    "\n",
    "        for i in range(len(fig_output_vector)):\n",
    "            self.wandb_run.log({f'test/{data_set_name}/{img_names[i]}/full_output': fig_output_vector[i]})\n",
    "            self.wandb_run.log({f'test/{data_set_name}/{img_names[i]}/subsetted_output': fig_subsetted_output_vector[i]})\n",
    "            self.wandb_run.log({f'test/{data_set_name}/{img_names[i]}/input': fig_input_vector[i]})\n",
    "\n",
    "\n",
    "\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "ResNet-based Keypoint Estimator\n",
    "\"\"\"\n",
    "class Fem_Configuration:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        self.temp = {\n",
    "            'train_data': '/home/sasank/Documents/GitRepos/Sasank_JTML_seg/data/3_2_22_fem/train_3_2_22_fem.csv',\n",
    "            'val_data': '/home/sasank/Documents/GitRepos/Sasank_JTML_seg/data/3_2_22_fem/val_3_2_22_fem.csv',\n",
    "            'test_data': '/home/sasank/Documents/GitRepos/Sasank_JTML_seg/data/3_2_22_fem/test_3_2_22_fem.csv'\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.init = {\n",
    "            'PROJECT_NAME': 'Keypoint Estimation',\n",
    "            'MODEL_NAME': 'Fem_64KP',\n",
    "            'RUN_NAME': time.strftime('%Y-%m-%d-%H-%M-%S'),\n",
    "            'WANDB_RUN_GROUP': 'Local',\n",
    "            'FAST_DEV_RUN': False,  # Runs inputted batches (True->1) and disables logging and some callbacks\n",
    "            'MAX_EPOCHS': 10,\n",
    "            'MAX_STEPS': -1,    # -1 means it will do all steps and be limited by epochs\n",
    "            'STRATEGY': None    # This is the training strategy. Should be 'ddp' for multi-GPU (like HPG)\n",
    "        }\n",
    "        self.etl = {\n",
    "            'RAW_DATA_FILE': -1,\n",
    "            'DATA_DIR': \"data\",\n",
    "            # Lol what is this?\n",
    "            'KEYPOINT_DIRECTORY': \"keypoints\",\n",
    "            'KEYPOINT_TXT_FILES': ['tib_KPlabels_16.txt'],\n",
    "            'VAL_SIZE':  0.2,       # looks sus\n",
    "            'TEST_SIZE': 0.01,      # I'm not sure these two mean what we think\n",
    "            #'random_state': np.random.randint(1,50)\n",
    "            # HHG2TG lol; deterministic to aid reproducibility\n",
    "            'RANDOM_STATE': 42,\n",
    "\n",
    "            'CUSTOM_TEST_SET': False,\n",
    "            'TEST_SET_NAME': '/my/test/set.csv'\n",
    "        }\n",
    "\n",
    "        self.dataset = {\n",
    "            'DATA_NAME': 'Ten_Dogs_64KP',\n",
    "            'SUBSET_PIXELS': True,\n",
    "            'IMAGE_HEIGHT': 1024,\n",
    "            'IMAGE_WIDTH': 1024,\n",
    "            'MODEL_TYPE': 'fem',        # how should we do this? not clear this is still best...\n",
    "            'CLASS_LABELS': {0: 'bone', 1: 'background'},\n",
    "            'NUM_KEY_POINTS': 64,\n",
    "            'IMG_CHANNELS': 1,      # Is this different from self.module['NUM_IMAGE_CHANNELS']\n",
    "            'STORE_DATA_RAM': False,\n",
    "            'IMAGE_THRESHOLD': 0,\n",
    "            'USE_ALBUMENTATIONS': False,\n",
    "\n",
    "            # What do these do?\n",
    "            'NUM_PRINT_IMG' : 1,\n",
    "            'KP_PLOT_RAD' : 3,\n",
    "\n",
    "            #'NUM_POINTS' : 128,\n",
    "\n",
    "            'GAUSSIAN_STDDEV' : 5,\n",
    "            'GAUSSIAN_AMP' : 1e3,\n",
    "\n",
    "            'STORE_DATA_RAM' : False,\n",
    "\n",
    "            'CROP_IMAGES' : False,\n",
    "            'CROP_MIN_X' : 0.29,\n",
    "            'CROP_MAX_X' : 0.84,\n",
    "            'CROP_MIN_Y' : 0.45,\n",
    "            'CROP_MAX_Y' : 0.95,\n",
    "            \n",
    "            'IMAGES_PER_GRID': 1,\n",
    "            'per_grid_image_count_height' : 1, \n",
    "            'per_grid_image_count_width' : 1\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        # segmentation_net_module needs to be below dataset because it uses dataset['IMG_CHANNELS']\n",
    "        self.keypoint_net_module = {\n",
    "            'NUM_KEY_POINTS': 128,\n",
    "            'NUM_IMG_CHANNELS': self.dataset['IMG_CHANNELS']\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.datamodule = {\n",
    "            'IMAGE_DIRECTORY': '/media/sasank/LinuxStorage/Dropbox (UFL)/Canine Kinematics Data/TPLO_Ten_Dogs_grids/',\n",
    "            'CKPT_FILE': None,\n",
    "            'USE_NAIVE_TEST_SET': False,\n",
    "            'BATCH_SIZE': 2,\n",
    "            'SHUFFLE': True,        # Only for training; for test and val this is set in the datamodule script to False\n",
    "            'NUM_WORKERS': 2,\n",
    "            'PIN_MEMORY': False\n",
    "            #'SUBSET_PIXELS': True - this is now in dataset\n",
    "        }\n",
    "\n",
    "\n",
    "        # hyperparameters for training\n",
    "        self.hparams = {\n",
    "            'LOAD_FROM_CHECKPOINT': False,\n",
    "            'learning_rate': 1e-3\n",
    "        }\n",
    "\n",
    "        #self.transform = None\n",
    "        self.transform = \\\n",
    "        A.Compose([\n",
    "            # Let's do only rigid transformations for now\n",
    "            A.HorizontalFlip(p=0.9),\n",
    "            A.VerticalFlip(p=0.9),\n",
    "            A.RandomRotate90(p=0.9),\n",
    "            A.Transpose(p=0.9),\n",
    "            #A.RandomGamma(always_apply=False, p = 0.5,gamma_limit=(10,300)),\n",
    "            #A.ShiftScaleRotate(always_apply = False, p = 0.5,shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-180,180), interpolation=0, border_mode=0, value=(0, 0, 0)),\n",
    "            #A.Blur(always_apply=False, blur_limit=(3, 10), p=0.2),\n",
    "            #A.Flip(always_apply=False, p=0.5),\n",
    "            #A.InvertImg(always_apply=False, p=0.5),\n",
    "            #A.MultiplicativeNoise(always_apply=False, p=0.25, multiplier=(0.1, 2), per_channel=True, elementwise=True)\n",
    "            #A.ElasticTransform(always_apply=False, p=0.85, alpha=0.5, sigma=150, alpha_affine=50.0, interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None, approximate=False),\n",
    "            #A.CoarseDropout(always_apply = False, p = 0.25, min_holes = 1, max_holes = 100, min_height = 25, max_height=25),\n",
    "        ],\n",
    "        p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "ResNet-based Keypoint Estimator\n",
    "\"\"\"\n",
    "class Tib_Configuration:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        self.temp = {\n",
    "            'train_data': '/home/sasank/Documents/GitRepos/Sasank_JTML_seg/data/3_2_22_fem/train_3_2_22_fem.csv',\n",
    "            'val_data': '/home/sasank/Documents/GitRepos/Sasank_JTML_seg/data/3_2_22_fem/val_3_2_22_fem.csv',\n",
    "            'test_data': '/home/sasank/Documents/GitRepos/Sasank_JTML_seg/data/3_2_22_fem/test_3_2_22_fem.csv'\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.init = {\n",
    "            'PROJECT_NAME': 'Keypoint Estimation',\n",
    "            'MODEL_NAME': 'Tib_64KP',\n",
    "            'RUN_NAME': time.strftime('%Y-%m-%d-%H-%M-%S'),\n",
    "            'WANDB_RUN_GROUP': 'Local',\n",
    "            'FAST_DEV_RUN': False,  # Runs inputted batches (True->1) and disables logging and some callbacks\n",
    "            'MAX_EPOCHS': 10,\n",
    "            'MAX_STEPS': -1,    # -1 means it will do all steps and be limited by epochs\n",
    "            'STRATEGY': None    # This is the training strategy. Should be 'ddp' for multi-GPU (like HPG)\n",
    "        }\n",
    "        self.etl = {\n",
    "            'RAW_DATA_FILE': -1,\n",
    "            'DATA_DIR': \"data\",\n",
    "            # Lol what is this?\n",
    "            'KEYPOINT_DIRECTORY': \"keypoints\",\n",
    "            'KEYPOINT_TXT_FILES': ['tib_KPlabels_16.txt'],\n",
    "            'VAL_SIZE':  0.2,       # looks sus\n",
    "            'TEST_SIZE': 0.01,      # I'm not sure these two mean what we think\n",
    "            #'random_state': np.random.randint(1,50)\n",
    "            # HHG2TG lol; deterministic to aid reproducibility\n",
    "            'RANDOM_STATE': 42,\n",
    "\n",
    "            'CUSTOM_TEST_SET': False,\n",
    "            'TEST_SET_NAME': '/my/test/set.csv'\n",
    "        }\n",
    "\n",
    "        self.dataset = {\n",
    "            'DATA_NAME': 'Ten_Dogs_64KP',\n",
    "            'SUBSET_PIXELS': True,\n",
    "            'IMAGE_HEIGHT': 1024,\n",
    "            'IMAGE_WIDTH': 1024,\n",
    "            'MODEL_TYPE': 'tib',        # how should we do this? not clear this is still best...\n",
    "            'CLASS_LABELS': {0: 'bone', 1: 'background'},\n",
    "            'NUM_KEY_POINTS': 64,\n",
    "            'IMG_CHANNELS': 1,      # Is this different from self.module['NUM_IMAGE_CHANNELS']\n",
    "            'STORE_DATA_RAM': False,\n",
    "            'IMAGE_THRESHOLD': 0,\n",
    "            'USE_ALBUMENTATIONS': False,\n",
    "\n",
    "            # What do these do?\n",
    "            'NUM_PRINT_IMG' : 1,\n",
    "            'KP_PLOT_RAD' : 3,\n",
    "\n",
    "            #'NUM_POINTS' : 128,\n",
    "\n",
    "            'GAUSSIAN_STDDEV' : 5,\n",
    "            'GAUSSIAN_AMP' : 1e3,\n",
    "\n",
    "            'STORE_DATA_RAM' : False,\n",
    "\n",
    "            'CROP_IMAGES' : False,\n",
    "            'CROP_MIN_X' : 0.29,\n",
    "            'CROP_MAX_X' : 0.84,\n",
    "            'CROP_MIN_Y' : 0.45,\n",
    "            'CROP_MAX_Y' : 0.95,\n",
    "            \n",
    "            'IMAGES_PER_GRID': 1,\n",
    "            'per_grid_image_count_height' : 1, \n",
    "            'per_grid_image_count_width' : 1\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        # segmentation_net_module needs to be below dataset because it uses dataset['IMG_CHANNELS']\n",
    "        self.keypoint_net_module = {\n",
    "            'NUM_KEY_POINTS': 128,\n",
    "            'NUM_IMG_CHANNELS': self.dataset['IMG_CHANNELS']\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        self.datamodule = {\n",
    "            'IMAGE_DIRECTORY': '/media/sasank/LinuxStorage/Dropbox (UFL)/Canine Kinematics Data/TPLO_Ten_Dogs_grids/',\n",
    "            'CKPT_FILE': None,\n",
    "            'USE_NAIVE_TEST_SET': False,\n",
    "            'BATCH_SIZE': 2,\n",
    "            'SHUFFLE': True,        # Only for training; for test and val this is set in the datamodule script to False\n",
    "            'NUM_WORKERS': 2,\n",
    "            'PIN_MEMORY': False\n",
    "            #'SUBSET_PIXELS': True - this is now in dataset\n",
    "        }\n",
    "\n",
    "\n",
    "        # hyperparameters for training\n",
    "        self.hparams = {\n",
    "            'LOAD_FROM_CHECKPOINT': False,\n",
    "            'learning_rate': 1e-3\n",
    "        }\n",
    "\n",
    "        #self.transform = None\n",
    "        self.transform = \\\n",
    "        A.Compose([\n",
    "            # Let's do only rigid transformations for now\n",
    "            A.HorizontalFlip(p=0.9),\n",
    "            A.VerticalFlip(p=0.9),\n",
    "            A.RandomRotate90(p=0.9),\n",
    "            A.Transpose(p=0.9),\n",
    "            #A.RandomGamma(always_apply=False, p = 0.5,gamma_limit=(10,300)),\n",
    "            #A.ShiftScaleRotate(always_apply = False, p = 0.5,shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-180,180), interpolation=0, border_mode=0, value=(0, 0, 0)),\n",
    "            #A.Blur(always_apply=False, blur_limit=(3, 10), p=0.2),\n",
    "            #A.Flip(always_apply=False, p=0.5),\n",
    "            #A.InvertImg(always_apply=False, p=0.5),\n",
    "            #A.MultiplicativeNoise(always_apply=False, p=0.25, multiplier=(0.1, 2), per_channel=True, elementwise=True)\n",
    "            #A.ElasticTransform(always_apply=False, p=0.85, alpha=0.5, sigma=150, alpha_affine=50.0, interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None, approximate=False),\n",
    "            #A.CoarseDropout(always_apply = False, p = 0.25, min_holes = 1, max_holes = 100, min_height = 25, max_height=25),\n",
    "        ],\n",
    "        p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "\n",
    "from loss import kp_loss\n",
    "\n",
    "\n",
    "\n",
    "class KeypointDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, config, evaluation_type, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (config): Dictionary of vital constants about data.\n",
    "            store_data_ram (boolean): Taken from config.\n",
    "            evaluation_type (string): Dataset evaluation type (must be 'training', 'validation', 'test', or 'naive')\n",
    "            num_points (int): Taken from config.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        # Create local copies of the arguments\n",
    "        self.config = config\n",
    "        self.num_points = self.config.dataset['NUM_KEY_POINTS']\n",
    "        self.transform = self.config.transform\n",
    "        \n",
    "        # Check that evaluation_type is valid and then store\n",
    "        if evaluation_type in ['train', 'val', 'test', 'naive']:\n",
    "            self.evaluation_type = evaluation_type\n",
    "        else:\n",
    "            raise Exception('Incorrect evaluation type! Must be either \\'train\\', \\'val\\', \\'test\\', or \\'naive\\'.')\n",
    "\n",
    "        # Load the data from the big_data CSV file into a pandas dataframe\n",
    "        #self.data = pd.read_csv(os.path.join(self.config.etl['DATA_DIR'], self.config.dataset['DATA_NAME'], self.evaluation_type + '_' + self.config.dataset['DATA_NAME'] + '.csv'))\n",
    "        self.data = pd.read_csv('/home/sasank/Documents/GitRepos/Stifle-Keypoints/data/Ten_Dogs_64KP/test_Ten_Dogs_64KP.csv')\n",
    "        # Print number of rows in the dataframe\n",
    "        print('Number of rows in the dataframe: ', len(self.data))\n",
    "        # Print 'Image address' of the first row\n",
    "        print('Image address of the first row: ', self.data.iloc[0]['Image address'])\n",
    "        # Print 'Image address' of the last row\n",
    "        print('Image address of the last row: ', self.data.iloc[-1]['Image address'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #idx += 1    # Add 1 because the first row is the column names\n",
    "\n",
    "        # Get the row of the dataframe\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Get the image name\n",
    "        image_name = row['Image address']\n",
    "\n",
    "        # Get the image\n",
    "        image = io.imread(os.path.join(self.config.datamodule['IMAGE_DIRECTORY'], image_name))\n",
    "        full_image = image             # Save full image (no subset_pixels) for visualization\n",
    "\n",
    "        # Get the keypoint labels and segmentation labels\n",
    "        if self.config.dataset['MODEL_TYPE'] == 'fem':\n",
    "            kp_label = row['Femur 2D KP points']\n",
    "            seg_label = io.imread(os.path.join(self.config.datamodule['IMAGE_DIRECTORY'], row['Fem label address']))\n",
    "        elif self.config.dataset['MODEL_TYPE'] == 'tib':\n",
    "            kp_label = row['Tibia 2D KP points']\n",
    "            seg_label = io.imread(os.path.join(self.config.datamodule['IMAGE_DIRECTORY'], row['Tib label address']))\n",
    "        else:\n",
    "            raise Exception('Incorrect model type! Must be either \\'fem\\' or \\'tib\\'.')\n",
    "\n",
    "        kp_label = kp_label[2:-2]\n",
    "        kp_label = kp_label.split(']\\n [')\n",
    "        kp_label = [np.array([float(x) for x in list(filter(None, kp.split(' ')))]) for kp in kp_label]\n",
    "        kp_label = np.array(kp_label)\n",
    "        \n",
    "        # * Subset Pixels\n",
    "        if self.config.dataset['SUBSET_PIXELS'] == True:\n",
    "            label_dst = np.zeros_like(seg_label)\n",
    "            label_normed = cv2.normalize(seg_label, label_dst, alpha = 0, beta = 1, norm_type = cv2.NORM_MINMAX)\n",
    "            seg_label = label_normed\n",
    "\n",
    "            kernel = np.ones((30,30), np.uint8)\n",
    "            label_dilated = cv2.dilate(seg_label, kernel, iterations = 5)\n",
    "            image_subsetted = cv2.multiply(label_dilated, image)\n",
    "            image = image_subsetted\n",
    "\n",
    "        image = torch.FloatTensor(image[None, :, :]) # Store as byte (to save space) then convert when called in __getitem__\n",
    "        full_image = torch.FloatTensor(full_image[None, :, :]) # Store as byte (to save space) then convert when called in __getitem__\n",
    "        seg_label = torch.FloatTensor(seg_label[None, :, :])\n",
    "        #kp_label = torch.FloatTensor(kp_label.reshape(-1))      # Reshape to 1D array so that it's 2*num_keypoints long\n",
    "        kp_label = torch.FloatTensor(kp_label)          # kp_label is of shape (num_keypoints, 2)\n",
    "        assert kp_label.shape == (self.num_points, 2), \"Keypoint label shape is incorrect!\"\n",
    "        #print(\"kp_label.shape:\")\n",
    "        #print(kp_label.shape)\n",
    "\n",
    "\n",
    "    \n",
    "        # Form sample and transform if necessary\n",
    "        sample = {'image': image,\n",
    "                    'img_name': image_name,\n",
    "                    'kp_label': kp_label,\n",
    "                    'seg_label': seg_label,\n",
    "                    'full_image': full_image}\n",
    "        assert self.transform is not None, \"Transforms not implemented yet!\"\n",
    "        if self.transform and self.config.dataset['USE_ALBUMENTATIONS'] == True:\n",
    "            sample = self.transform(sample)     # TODO: Implement transforms. Seems like we'll have to reshape the keypoints to be num_keypoints x 2 instead of 2*num_keypoints x 1\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataframe:  365\n",
      "Image address of the first row:  grid_Calib file test_000000001769.tif\n",
      "Image address of the last row:  grid_Calib file test_000000000452.tif\n",
      "Number of rows in the dataframe:  365\n",
      "Image address of the first row:  grid_Calib file test_000000001769.tif\n",
      "Image address of the last row:  grid_Calib file test_000000000452.tif\n"
     ]
    }
   ],
   "source": [
    "fem_config = Fem_Configuration()\n",
    "tib_config = Tib_Configuration()\n",
    "fem_dataset = KeypointDataset(fem_config, 'test')\n",
    "tib_dataset = KeypointDataset(tib_config, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.7.6 to v1.9.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../checkpoints/Fem64_200Epochs_Resnet152.ckpt`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.7.6 to v1.9.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../checkpoints/Tib64_200Epochs_Resnet152.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in fem_dataset is :\n",
      "365\n",
      "Number of data rows in the csv file is :\n",
      "365\n",
      "Number of batches in the dataloader is :\n",
      "365\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasank/miniconda3/envs/jtml/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#from lit_KPResNet import KeypointNetModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# later on, will need to change to accept outputs from updated model (compatible with Albumentations), where the kps have been multiplied by 1024 and the y reversed (by 1024 - y)\n",
    "\n",
    "FEM_CHECKPOINT_PATH = '/home/sasank/Documents/GitRepos/Stifle-Keypoints/checkpoints/Fem64_200Epochs_Resnet152.ckpt'\n",
    "TIB_CHECKPOINT_PATH = '/home/sasank/Documents/GitRepos/Stifle-Keypoints/checkpoints/Tib64_200Epochs_Resnet152.ckpt'\n",
    "fem_model = KeypointNetModule.load_from_checkpoint(FEM_CHECKPOINT_PATH, config=fem_config, wandb_run = None)\n",
    "tib_model = KeypointNetModule.load_from_checkpoint(TIB_CHECKPOINT_PATH, config=tib_config, wandb_run = None)\n",
    "\n",
    "# ! This csv_file does not actually get used to get the outputs. It is only used to get the number of entries in the csv file.\n",
    "# ! Edit the csv file in the Dataset object above to change the csv file used to get the outputs.\n",
    "csv_file = pd.read_csv('/home/sasank/Documents/GitRepos/PnP-Solver/kp_estimates/test_set_64KP.csv', index_col=0)\n",
    "\n",
    "# Number of entries in fem_dataset\n",
    "print('Number of entries in fem_dataset is :')\n",
    "print(len(fem_dataset))\n",
    "\n",
    "print('Number of data rows in the csv file is :')\n",
    "print(len(csv_file))\n",
    "\n",
    "fem_dataloader = DataLoader(fem_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "tib_dataloader = DataLoader(tib_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Iterate through the dataloader and get the outputs\n",
    "\n",
    "print('Number of batches in the dataloader is :')\n",
    "print(len(fem_dataloader))\n",
    "\n",
    "np.set_printoptions(suppress=True)      # Don't use scientific notation\n",
    "\n",
    "####### ONNX EXPORT #######\n",
    "#\"\"\"\n",
    "fem_model.eval()\n",
    "fem_model.to('cpu')\n",
    "fem_model = fem_model.cpu()\n",
    "dummy_input = torch.randn(1, 1, 1024, 1024)\n",
    "torch.onnx.export(fem_model, dummy_input, \"Fem64_200Epochs_R152_nonverbose.onnx\", verbose=False)\n",
    "\n",
    "tib_model.eval()\n",
    "tib_model.to('cpu')\n",
    "tib_model = tib_model.cpu()\n",
    "dummy_input = torch.randn(1, 1, 1024, 1024)\n",
    "torch.onnx.export(tib_model, dummy_input, \"Tib64_200Epochs_R152_nonverbose.onnx\", verbose=False)\n",
    "\n",
    "sys.exit(0)\n",
    "#\"\"\"\n",
    "###########################\n",
    "\n",
    "# Femur\n",
    "for i, sample in enumerate(fem_dataloader):\n",
    "    image = sample['image']\n",
    "    img_name = sample['img_name'][0]\n",
    "    #print('image name is ' + img_name)\n",
    "    #print('image name is ' + img_name[0])\n",
    "    #image.unsqueeze_(0)\n",
    "    output = fem_model(image)\n",
    "\n",
    "    # We will plot the outputs, which are 2D keypoints, to self.csv_file in the row that corresponds to the image name\n",
    "    keypoints = output.detach().cpu().numpy()\n",
    "    # Make the keypoints a 2D array of shape (num_keypoints, 2)\n",
    "    keypoints = keypoints.reshape(fem_config.dataset['NUM_KEY_POINTS'], 2)\n",
    "\n",
    "    # Find the row that corresponds to the image name and put the keypoints there\n",
    "    csv_file.loc[csv_file['Image address'] == img_name, 'Femur PR KP points'] = str(keypoints)\n",
    "\n",
    "# Tibia\n",
    "for i, sample in enumerate(tib_dataloader):\n",
    "    image = sample['image']\n",
    "    img_name = sample['img_name'][0]\n",
    "    #image.unsqueeze_(0)\n",
    "    output = tib_model(image)\n",
    "\n",
    "    # We will plot the outputs, which are 2D keypoints, to self.csv_file in the row that corresponds to the image name\n",
    "    keypoints = output.detach().cpu().numpy()\n",
    "    # Make the keypoints a 2D array of shape (num_keypoints, 2)\n",
    "    keypoints = keypoints.reshape(tib_config.dataset['NUM_KEY_POINTS'], 2)\n",
    "\n",
    "    # Find the row that corresponds to the image name and put the keypoints there\n",
    "    csv_file.loc[csv_file['Image address'] == img_name, 'Tibia PR KP points'] = str(keypoints)\n",
    "\n",
    "csv_file.to_csv('/home/sasank/Documents/GitRepos/PnP-Solver/kp_estimates/test_predicted_R152_asdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Image address, Fem label address, Tib label address, Patient name, Patient number, Session name, Session number, Movement name, Movement number, Principal distance, X offset, Y offset, Scale, Calib translation, Calib y_hat, Calib z_hat, Femur STL, Tibia STL, Femur kinematics (x_tran, y_tran, z_tran, z_rot, x_rot, y_rot), Tibia kinematics (x_tran, y_tran, z_tran, z_rot, x_rot, y_rot), Femur 3D KP points, Tibia 3D KP points, Femur 2D KP points, Tibia 2D KP points]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n",
      "[2 3]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/sasank/Documents/GitRepos/Stifle-Keypoints/data/personal_val.csv', index_col=0)\n",
    "\n",
    "# Check if there are rows with Patient number = 3 and Session number = 1\n",
    "print(data.loc[(data['Patient number'] == 3) & (data['Session number'] == 1)])\n",
    "\n",
    "# Print unique values of Session number for Patient number = 3\n",
    "print(data.loc[data['Patient number'] == 3]['Session number'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m PRINT_DIR \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/media/sasank/LinuxStorage/Dropbox (UFL)/Canine Kinematics Data/image_check/pruned_tib/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Iterate through the dataset and print the shapes of the images and labels\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)):\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mimage number: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i))\n\u001b[1;32m      5\u001b[0m     batch \u001b[39m=\u001b[39m dataset[i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "PRINT_DIR = '/media/sasank/LinuxStorage/Dropbox (UFL)/Canine Kinematics Data/image_check/pruned_tib/'\n",
    "# Iterate through the dataset and print the shapes of the images and labels\n",
    "for i in range(len(dataset)):\n",
    "    print(\"image number: \" + str(i))\n",
    "    batch = dataset[i]\n",
    "    images = batch['image']\n",
    "    kp_labels = batch['kp_label']\n",
    "    img_names = batch['img_name']\n",
    "    title = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    num_images = images.shape[0]\n",
    "    num_keypoints = kp_labels.shape[0]\n",
    "    assert num_keypoints == 64, \"Number of keypoints is incorrect!\"\n",
    "    images = images.cpu()\n",
    "    kp_labels = kp_labels.cpu()\n",
    "    kp_labels = kp_labels.numpy()\n",
    "\n",
    "    output_image_vector = []\n",
    "\n",
    "    for i in range(0, num_images):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10), squeeze=False)\n",
    "\n",
    "        # ! TODO: Is this the right way to do this? Is there something wrong here with offsets or something?\n",
    "        #kp_labels[i][:, 0] = +1 * kp_labels[i][:, 0] * 1024\n",
    "        kp_labels[:, 0] = +1 * kp_labels[:, 0] * 1024\n",
    "        #kp_labels[i][:, 1] = -1 * kp_labels[i][:, 1] * 1024 + 1024\n",
    "        kp_labels[:, 1] = -1 * kp_labels[:, 1] * 1024 + 1024\n",
    "        # Do some stuff so that img is shown correctly\n",
    "        img = images.numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))  # Transpose the output so that it's the same way as img\n",
    "        img = np.dstack((img, img, img))    # Make it 3 channels\n",
    "        ax[0][0].imshow((img * 255).astype(np.uint8))  # The multiplying by 255 and stuff is so it doesn't get clipped or something\n",
    "\n",
    "        for j in range(num_keypoints):\n",
    "            #ax[0][0].text(labels[i][j, 0], labels[i][j, 1], str(j), color='m')        # Silenced this for now since we have 64 keypoints\n",
    "            ax[0][0].plot(kp_labels[j, 0], kp_labels[j, 1], color='orange', marker='.', markersize=5)\n",
    "            #ax[0][0].plot([kp_labels[i][j, 0], kp_preds[i][j, 0]], [labels[i][j, 1], preds[i][j, 1]], color='limegreen', linestyle='-')\n",
    "        image_name = img_names.split('/')[-1]    # Format img_names[i] so that only the part after the last '/' is shown\n",
    "        ax[0][0].set_title(title + ' {}'.format(image_name))\n",
    "        fig.savefig(os.path.join(PRINT_DIR, image_name))\n",
    "        output_image_vector.append(fig)\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
