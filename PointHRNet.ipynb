{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointHRNet\n",
    "Uses the High Resolution Net (HRNet) from Microsoft Research Asia to predict 'heat maps' for key points. Input data is a grayscale image and the input label for a datapoint is two numbers (x,y) that have been normalized to fit on [0,1] corresponding to the location on the input image. This input label is then transformed into an image (the same size as the data) with a 2D Gaussian centered at the key point (with a custom standard deviation, typically around 1 pixel). One can have multiple key points per image, which will all get their own heat maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "Using PyTorch and Torchvision, Image Processing SciKit, Matplotlib, Time, Sys, Glob, OS, Math, Random, HRNet (leoxiaobin), NumPy, OpenCV, Itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from pose_hrnet import PoseHighResolutionNet\n",
    "import numpy as np\n",
    "import cv2\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants/Parameters\n",
    "Constants/parameters that can be changed (although most variables must reflect the input data parameters, like image size, or the model and loss function parameters, like the GPU data types). Data is in a 1x1 image format and the labels are in a text file with the name of the file on a line, then a (x,y) location point below it, then finally this is transformed into an image heat map with a 2D Gaussian around the (x,y) location. If there are more than one location points being trained, these are on seperate lines.\n",
    "\n",
    "#### Note 1:\n",
    "If not loading all images into memory on the creation of the dataset class, it is fastest to use a 1 image x 1 image grid. Should be fine on the OS unless using > ~1 million images. If loading to RAM, should be fine using a larger grid, however\n",
    "the initial process of loading will be slower (the statement in this sentence needs to be tested).\n",
    "\n",
    "#### Note 2:  \n",
    "The fastest method seems to be storing the data in ram (STORE_DATA_RAM = True) and keeping pinned memory off (PIN_MEMORY = False) and no multithreading on the CPU (NUM_WORKERS = 0). However, if RAM storage is running out due to a very large dataset, then turn off storing data in RAM (STORE_DATA_RAM = False), turn on pinned memory (PIN_MEMORY = True), and add multi-threading (NUM_WORKERS = 2 seems to be a sweet spot although this could vary and should be experimented with if RAM storage is turned off). This won't be as fast as using the RAM storage method with no pinning and no multithreading, but it should still be decently fast. If running with multi-threading, don't use the Jupyter enviroment - instead run straight from Python.\n",
    "\n",
    "#### Note 3:  \n",
    "The GPU tensor data types are based on the input to the model (architecture) and the input to the loss function (which is a function of the output of the model and the labels). Make these compatible with the model (architecture) input and loss function requirements to avoid slow, implicit recasts (at least I think that is what's happening...). These types should be experimented with (or alternatively go read the documentation) to find the appropriate (which should also be the fastest) types when using a different architecture and/or loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data constants\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "DATA_HOME_DIR = 'C:/Users/pflood/Desktop/JTAML/Python/PyTorch/PointHRNet/dataEmil'\n",
    "MODEL_TYPE = 'points'\n",
    "\n",
    "# Data loader parameters\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "NUM_WORKERS = 0 # If not storing data in RAM, try turning this on (2 is potentially a good number)\n",
    "PIN_MEMORY = False # Slower if storing in RAM (so keep False), but seems to make faster if using workers and not storing in RAM \n",
    "\n",
    "# Data storage parameters\n",
    "'''\n",
    "STORE_DATA_RAM loads images and labels to ram on creation of dataset class if True.\n",
    "Much faster if room for it. Keep NUM_WORKERS = 0 and PIN_MEMORY = False, if turned on (STORE_DATA_RAM = True)\n",
    "'''\n",
    "STORE_DATA_RAM = True\n",
    "\n",
    "# GPU Tensor Data Types\n",
    "'''\n",
    "These are the data types for the images and their corresponding labels when both are pushed to the GPU(s).\n",
    "Make these compatible with the model (architecture) input and loss function requirements to avoid slow, implicit recasts.\n",
    "Might need to play around or read documentation to figure this out when changing architectures and/or loss function.\n",
    "'''\n",
    "IMAGES_GPU_DATA_TYPE = torch.FloatTensor\n",
    "\n",
    "# Training Parameters\n",
    "MAX_EPOCHS = sys.maxsize # How many epochs to run training cycle, sys.maxsize gives 2^63 which will essentially run forever\n",
    "\n",
    "# Validation Parameters\n",
    "'''\n",
    "Don't want too big as this appears to take up additional GPU space, however too small will lead to latencies in code. 4 is good.\n",
    "'''\n",
    "VALIDATION_BATCH_SIZE = 4\n",
    "\n",
    "# Image Printing Parameters\n",
    "'''\n",
    "NUM_PRINT_IMG is the number of sampled images (number of rows).\n",
    "KP_PLOT_RAD is the radius in pixels of the plotted key points.\n",
    "'''\n",
    "NUM_PRINT_IMG = 1\n",
    "KP_PLOT_RAD = 3\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME ='EMIL_POINTS_VER_1'\n",
    "\n",
    "# Loading from previous checkpoint?\n",
    "LOAD_FROM_CHECKPOINT = True\n",
    "\n",
    "# Number of channels in input image (Grayscale = 1, RGB = 3, etc.)\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "# Number of Feature Points in input image (assumes all images have this many points)\n",
    "NUM_POINTS = 23\n",
    "\n",
    "# Standard Deviation for 2D Gaussian Heat Map Label (in pixel units on a Euclidean plane)\n",
    "GAUSSIAN_STDEV_HEATMAP = 5\n",
    "GAUSSIAN_AMP = 1e6\n",
    "\n",
    "# Test Output Original Directory from C++\n",
    "TEST_OUTPUT_DIR = 'C:/Users/pflood/Desktop/JTAML/RealShoulderData/AkiraOutput43PointsLeftScapulaTest'\n",
    "\n",
    "# Test Original Directory from C++\n",
    "TEST_ORIGINAL_DIR = 'C:/Users/pflood/Desktop/JTAML/RealShoulderData/AkiraOrganizedLeftTest'\n",
    "\n",
    "# Crop to Specified Range on Images (Way of Feeding In Higher Resolution Images)\n",
    "CROP_IMAGES = False\n",
    "CROP_MIN_X = 0.29\n",
    "CROP_MAX_X = 0.84\n",
    "CROP_MIN_Y = 0.45\n",
    "CROP_MAX_Y = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants/Parameters Dictionaries\n",
    "Dictionaries of certain parameters and constants. This does not have to be edited if solely adjusting the values of the above constants/parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the constants (IMAGES PER GRID KEPT CONSTANT AT 1)\n",
    "data_constants = {'image_height':IMAGE_HEIGHT, 'image_width':IMAGE_WIDTH,\\\n",
    "            'per_grid_image_count_height':1, 'per_grid_image_count_width':1,\\\n",
    "            'images_per_grid':1,\n",
    "            'data_home_dir':DATA_HOME_DIR, 'model_type':MODEL_TYPE}\n",
    "\n",
    "# Dictionary of parameters to be fed to data loader\n",
    "data_loader_parameters = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE,'num_workers': NUM_WORKERS,'pin_memory': PIN_MEMORY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gaussian Heatmaps\n",
    "Create gaussian heat maps centered at points with GAUSSIAN_STDEV_HEATMAP standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_heatmap(point_list, img_h, img_w):\n",
    "        '''\n",
    "        Takes list of (x,y) points and returns a list of gaussian heat maps centered at the respective points.\n",
    "        '''\n",
    "        heat_maps = []\n",
    "        for point in point_list:\n",
    "            g_x = np.linspace(0.5, img_w - 0.5, img_w)\n",
    "            g_y = np.linspace(0.5, img_h - 0.5, img_h)\n",
    "            g_x, g_y = np.meshgrid(g_x, g_y)\n",
    "            g_z = GAUSSIAN_AMP/(2*np.pi*GAUSSIAN_STDEV_HEATMAP**2) *np.exp(\n",
    "                #-(((g_x - math.floor(point[0]*img_w)+0.5)**2 + (g_y - math.floor(\n",
    "                #    img_h*(1 - point[1]))+0.5)**2) /(2*GAUSSIAN_STDEV_HEATMAP**2))) # ADDED FLOOR TO MAKE SURE GAUSSIAN ALWAYS PEAKS ON PIXEL\n",
    "                -(((g_x - point[0]*img_w)**2 + (g_y - img_h*(1 - point[1]))**2) /(2*GAUSSIAN_STDEV_HEATMAP**2)))\n",
    "            heat_maps.append(g_z)\n",
    "        return np.stack( heat_maps, axis=0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Feature Point Dataset Class\n",
    "Dataset class for point detection problems where images are arranged in 1x1 grids.\n",
    "\n",
    "#### Note 1:\n",
    "The model is fit on a training dataset. This trained model is used to predict responses in a validation dataset.\n",
    "This gives an unbiased evaluation of the model fitted on the training dataset, and we can adjust the hyperparameters (which\n",
    "includes parameters like batch size or even the types of images used as training data) to maximize performance on the validation\n",
    "dataset. To get an unbiased evaluation with respect to the hyperparameters we further evaluate on a test dataset. However,\n",
    "note that we save the model which peforms best on the validation dataset. The test dataset purely gives us a final evaluation. we denote the three evaluation type classes as: 'training', 'validation', and 'test'.\n",
    "\n",
    "#### Note 2:\n",
    "It is assumed that the labels sit in a file in a seperate folder found at %DATA_HOME_DIR%/labels/%MODEL_TYPE%. For example, on a \"fem\" model, the label file will be at %DATA_HOME_DIR%/labels/fem and called fem_labels.txt. Note that the corresponding images are further categorized into folders based on evaluation type. It is also assumed that all grids have the same image width, image height, grid width of 1, grid height of 1, and model type (i.e. the same data constants). Note that each image grid should appear in exactly one of the three different evaluation type directories (e.g. we don't use an image in the validation set if it already appears in the training set). These directories are given by %DATA_HOME_DIR%/%evaluation type%. Different grids must have different names even across different evaluation types. For example, we cannot have a grid45.tif in both the training set and test set, even if both grid45.tif files contain different images.In the label file, the image name will take up a line, then for each NUM_POINTS there will be a line after the image name that corresponds to the location of a feature point. Across different images this order is maintained (i.e. the ith point after an image is the same type of feature, say humerus head, across all images). The image grids only need to have unique names and can be called anything (although they need to be .tif files). \n",
    "\n",
    "#### Note 2:\n",
    "The __getitem__ method always returns the image and label tensors in the data type that the neural network and loss function need. If storing in RAM the images and labels are stored as bytes in RAM and then when __getitem__ is called they are cast to their required data types. If not storing in RAM, when the images and labels are generated, they are assigned their required data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePointDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Feature Point Dataset.\n",
    "    1. Grayscale images are stored in grids to reduce the number of file paths.\n",
    "    2. Labels are read in as NUM_POINTS 2-tuples per image stored in a text file, then transformed to 2D Gaussian heat maps.\n",
    "        a). Labels should be normalized in the text file already.\n",
    "        b). The label text file is stored in the %data_home_dir%/labels/%model_type% folder and is always named\n",
    "        '%model_type%_labels.txt'.\n",
    "        c). Checks will be performed to ensure that each 1x1 grid in %data_home_dir%/%evaluation_type% has a corresponding label, \n",
    "        else the class will throw an exception.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_constants, store_data_ram, evaluation_type, num_points, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_constants (dictionary): Dictionary of vital constants about data.\n",
    "            store_data_ram (boolean): Choose to store data in RAM for faster processing, or save RAM at a cost in performance.\n",
    "            evaluation_type (string): Dataset evaluation type (must be 'training', 'validation', or 'test')\n",
    "            num_points (int): Number of feature points per image.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        # Local copy of data_constants\n",
    "        self.data_constants = data_constants\n",
    "        \n",
    "        # Local copy of num_points\n",
    "        self.num_points = num_points\n",
    "        \n",
    "        # Check that evaluation_type is valid and then store\n",
    "        if evaluation_type in ['training', 'validation', 'test']:\n",
    "            self.evaluation_type = evaluation_type\n",
    "        else:\n",
    "            raise Exception('Incorrect evaluation type! Must be either \\'training\\', \\'validation\\', or \\'test\\'.')\n",
    "        \n",
    "        # Create full paths to all grids and their Labels\n",
    "        '''\n",
    "        Search %data_home_dir%/%evaluation_type%/ for all .tif files. \n",
    "        Check that properly named label text file in %data_home_dir%/labels/%model_type%/ exists for the given model type.\n",
    "        Throw an exception if this is not the case.\n",
    "        '''\n",
    "        image_grids = glob.glob1(self.data_constants['data_home_dir'] + '/' + self.evaluation_type, \"*.tif\") # Just name and .tif extension\n",
    "        self.grids_fullpaths = [self.data_constants['data_home_dir']  + '/' + self.evaluation_type + '/' +\\\n",
    "                      image_grid_name for image_grid_name in image_grids]\n",
    "            \n",
    "        # Calculate grid count\n",
    "        self.grid_count = len(self.grids_fullpaths)\n",
    "        \n",
    "        # Check that label text file exists\n",
    "        self.label_text_file = self.data_constants['data_home_dir']  + '/labels/' + self.data_constants['model_type']\\\n",
    "                                     + '/' + self.data_constants['model_type'] + '_KPlabels.txt'\n",
    "        if os.path.isfile(self.label_text_file) == False:\n",
    "            raise Exception('Error, cannot find file: ' + self.label_text_file)\n",
    "        \n",
    "        # Read in n x num_points x 2 list of point locations and n length list of associated file names\n",
    "        raw_text_list = [line.rstrip('\\n') for line in open(self.label_text_file)]\n",
    "        # Check that the raw text_list has at least length (num_points + 1)*self.grid_count\n",
    "        if len(raw_text_list) < (num_points + 1)*self.grid_count:\n",
    "            raise Exception('Error, the text file only contains ' + str(len(raw_text_list)) + ' lines!')\n",
    "        # Take Out The File Names\n",
    "        text_filenames = raw_text_list[0::(num_points + 1)]\n",
    "        del raw_text_list[0::(num_points + 1)]\n",
    "        # Organize Data in Same Order as Image Grids, If No Match Throw Error\n",
    "        label_str_data = []\n",
    "        data_point_strings = [line.split(',') for line in raw_text_list]\n",
    "        for image_scroll_idx in range(0,len(image_grids)):\n",
    "            for text_fname_idx in range(0,len(text_filenames)):\n",
    "                if image_grids[image_scroll_idx] == text_filenames[text_fname_idx]:\n",
    "                    label_str_data.append(data_point_strings[(text_fname_idx*num_points):(text_fname_idx*num_points + num_points)])\n",
    "                    break\n",
    "                if text_fname_idx == (len(text_filenames) - 1):\n",
    "                    raise Exception('Error, could not find label for ' + image_grids[image_scroll_idx] + '!')\n",
    "        # Convert to Numpy Array\n",
    "        self.label_point_data = np.array(label_str_data, dtype=float)\n",
    "        \n",
    "        # Error check:\n",
    "        # self.grid_fullpaths is the data input (images) of length 'n' and self.label_point_data are the labels\n",
    "        # Make sure self.label_point_data has size n x num_points x 2\n",
    "        if self.label_point_data.shape != (len(self.grids_fullpaths),num_points,2):\n",
    "                     raise Exception('Error, label data array has shape ' + str(self.label_point_data.shape) + '!')\n",
    "        \n",
    "        # Optional transform\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Store image tensors and label tensors to CPU RAM option (should be faster as long as there is room in the RAM)\n",
    "        self.store_data_ram = store_data_ram\n",
    "        self.data_storage = []\n",
    "        if self.store_data_ram:\n",
    "            for idx in range(self.grid_count*self.data_constants['images_per_grid']): # Total number of images\n",
    "                self.data_storage.append(self.read_in_data(idx))\n",
    "                \n",
    "        # Print successful initialization message\n",
    "        print (\"Successfully initialized \" + self.evaluation_type + ' dataset!')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.grid_count*self.data_constants['images_per_grid'] # Total number of images in data type (n)\n",
    "    \n",
    "    def read_in_data(self, idx):\n",
    "        # Read in image grid\n",
    "        grid_idx = idx//self.data_constants['images_per_grid']\n",
    "        grid_image = io.imread(self.grids_fullpaths[grid_idx], as_gray=True)\n",
    "        \n",
    "        # Extract image from grid using top-left to bottom-right ordering\n",
    "        idx_in_grid = idx%self.data_constants['images_per_grid']\n",
    "        img_top_row_idx = (idx_in_grid//self.data_constants['per_grid_image_count_width'])*self.data_constants['image_height']\n",
    "        img_left_col_idx = (idx_in_grid%self.data_constants['per_grid_image_count_width'])*self.data_constants['image_width']\n",
    "        image = grid_image[img_top_row_idx:img_top_row_idx + self.data_constants['image_height'],\\\n",
    "                          img_left_col_idx:img_left_col_idx + self.data_constants['image_width']]\n",
    "        \n",
    "        # Label should always be in [0,1] format when read in and then transformed into Gaussian heatmap       \n",
    "        # In PyTorch, images are represented as [channels, height, width] so must add 1 channel dimension\n",
    "        if CROP_IMAGES:\n",
    "            LEFT_X_PIX = math.floor(CROP_MIN_X*IMAGE_WIDTH)\n",
    "            RIGHT_X_PIX = math.ceil(CROP_MAX_X*IMAGE_WIDTH)\n",
    "            LEFT_X_PIX -= (32 - ((RIGHT_X_PIX - LEFT_X_PIX) % 32)) # So Width is Divisible by 32\n",
    "            BOT_Y_PIX = math.floor(CROP_MIN_Y*IMAGE_HEIGHT)\n",
    "            TOP_Y_PIX = math.ceil(CROP_MAX_Y*IMAGE_HEIGHT)\n",
    "            BOT_Y_PIX -= (32 - ((TOP_Y_PIX - BOT_Y_PIX) % 32)) # So Height is Divisible by 32\n",
    "            image = torch.ByteTensor(image[None, (IMAGE_HEIGHT - TOP_Y_PIX):(IMAGE_HEIGHT-BOT_Y_PIX), LEFT_X_PIX:RIGHT_X_PIX]) # Store as byte (to save space) then convert\n",
    "            cropped_point_list = [] # Need to Rescale Points\n",
    "            for orig_point in self.label_point_data[idx]:\n",
    "                cropped_point_list.append([(orig_point[0]*IMAGE_WIDTH - LEFT_X_PIX)/(RIGHT_X_PIX-LEFT_X_PIX),\\\n",
    "                                           (orig_point[1]*IMAGE_HEIGHT - BOT_Y_PIX)/(TOP_Y_PIX-BOT_Y_PIX)])\n",
    "            label = torch.FloatTensor(create_gaussian_heatmap(cropped_point_list, TOP_Y_PIX-BOT_Y_PIX, RIGHT_X_PIX-LEFT_X_PIX))\n",
    "            #image = torch.zeros([1,TOP_Y_PIX-BOT_Y_PIX, RIGHT_X_PIX-LEFT_X_PIX],dtype=torch.uint8)\n",
    "            #label = torch.zeros([NUM_POINTS,TOP_Y_PIX-BOT_Y_PIX, RIGHT_X_PIX-LEFT_X_PIX], dtype=torch.float)\n",
    "        else:\n",
    "            image = torch.ByteTensor(image[None, :, :]) # Store as byte (to save space) then convert when called in __getitem__\n",
    "            label = torch.FloatTensor(create_gaussian_heatmap(self.label_point_data[idx], IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Form sample and transform if necessary\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.store_data_ram:\n",
    "            return {'image': self.data_storage[idx]['image'].type(IMAGES_GPU_DATA_TYPE), 'label':\\\n",
    "                   self.data_storage[idx]['label']}\n",
    "        else:\n",
    "            # Read in image grid\n",
    "            grid_idx = idx//self.data_constants['images_per_grid']\n",
    "            grid_image = io.imread(self.grids_fullpaths[grid_idx], as_gray=True)\n",
    "\n",
    "            # Extract image from grid using top-left to bottom-right ordering\n",
    "            idx_in_grid = idx%self.data_constants['images_per_grid']\n",
    "            img_top_row_idx = (idx_in_grid//self.data_constants['per_grid_image_count_width'])*self.data_constants['image_height']\n",
    "            img_left_col_idx = (idx_in_grid%self.data_constants['per_grid_image_count_width'])*self.data_constants['image_width']\n",
    "            image = grid_image[img_top_row_idx:img_top_row_idx + self.data_constants['image_height'],\\\n",
    "                              img_left_col_idx:img_left_col_idx + self.data_constants['image_width']]\n",
    "            \n",
    "            # Label should always be in [0,1] format when read in and then transformed into Gaussian heatmap       \n",
    "            # In PyTorch, images are represented as [channels, height, width] so must add 1 channel dimension\n",
    "            if CROP_IMAGES:\n",
    "                LEFT_X_PIX = math.floor(CROP_MIN_X*IMAGE_WIDTH)\n",
    "                RIGHT_X_PIX = math.ceil(CROP_MAX_X*IMAGE_WIDTH)\n",
    "                LEFT_X_PIX -= (32 - ((RIGHT_X_PIX - LEFT_X_PIX) % 32)) # So Width is Divisible by 32\n",
    "                BOT_Y_PIX = math.floor(CROP_MIN_Y*IMAGE_HEIGHT)\n",
    "                TOP_Y_PIX = math.ceil(CROP_MAX_Y*IMAGE_HEIGHT)\n",
    "                BOT_Y_PIX -= (32 - ((TOP_Y_PIX - BOT_Y_PIX) % 32)) # So Height is Divisible by 32\n",
    "                image = torch.ByteTensor(image[None, (IMAGE_HEIGHT - TOP_Y_PIX):(IMAGE_HEIGHT-BOT_Y_PIX), LEFT_X_PIX:RIGHT_X_PIX]) # Store as byte (to save space) then convert\n",
    "                cropped_point_list = [] # Need to Rescale Points\n",
    "                for orig_point in self.label_point_data[idx]:\n",
    "                    cropped_point_list.append([(orig_point[0]*IMAGE_WIDTH - LEFT_X_PIX)/(RIGHT_X_PIX-LEFT_X_PIX),\\\n",
    "                                               (orig_point[1]*IMAGE_HEIGHT - BOT_Y_PIX)/(TOP_Y_PIX-BOT_Y_PIX)])\n",
    "                label = torch.FloatTensor(create_gaussian_heatmap(cropped_point_list, TOP_Y_PIX-BOT_Y_PIX, RIGHT_X_PIX-LEFT_X_PIX))\n",
    "            else:\n",
    "                image = torch.ByteTensor(image[None, :, :]) # Store as byte (to save space) then convert when called in __getitem__\n",
    "                label = torch.FloatTensor(create_gaussian_heatmap(self.label_point_data[idx], IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "            # Form sample and transform if necessary\n",
    "            sample = {'image': image, 'label': label}\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Image/Prediction/Overlay Function\n",
    "Function that is used to plot a sample of images from the validation batch and the corresponding true key points (in cyan), labels for the true key points (in green), the predicted version of the key points (in red), and a yellow line between the predicted and true key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(validation_image_batch, validation_label_batch, validation_output_batch, number_images_print):\n",
    "    '''\n",
    "    Plots a sample of images from the validation batch and the corresponding true key points (in cyan),\n",
    "    labels for the true key points (in green), the predicted version of the key points (in red),\n",
    "    and a yellow line between the predicted and true key point.\n",
    "    '''\n",
    "    # Check rows is less than validation set size\n",
    "    if (number_images_print > len(validation_image_batch)):\n",
    "        number_images_print = len(validation_image_batch)\n",
    "        print('Warning: Attempted to print more images than the validation batch contains!')\n",
    "        print('Only printing',number_images_print,'image(s).')\n",
    "    sample_image_indices = random.sample(range(0, len(validation_image_batch)), number_images_print)\n",
    " \n",
    "    # Plot Image With Key Points, Labels, Guesses for Key Points, Line between Each Actual KP and Guess KP\n",
    "    for img_to_print_idx in sample_image_indices:\n",
    "        plt.figure(figsize=(48,(IMAGE_HEIGHT/IMAGE_WIDTH)*48))\n",
    "        I = validation_image_batch[img_to_print_idx].to(\"cpu\").type(torch.float32)\n",
    "        L_heatmaps = (validation_label_batch[img_to_print_idx].to(\"cpu\")).type(torch.float32)\n",
    "        # Convert Heatmaps to Points By Taking The Maximum For Each Keypoint (Labels)\n",
    "        L = []\n",
    "        for kp_ind in range(0,L_heatmaps.shape[0]):\n",
    "            Lm_ind = torch.argmax(L_heatmaps[kp_ind])\n",
    "            Lx_ind = Lm_ind.item() % L_heatmaps[kp_ind].shape[1]\n",
    "            Ly_ind = Lm_ind.item() // L_heatmaps[kp_ind].shape[1] # Indexed by top left corner (same as OpenCV)\n",
    "            L.append([Lx_ind, Ly_ind])\n",
    "        G_heatmaps = (validation_output_batch[img_to_print_idx].to(\"cpu\")).type(torch.float32)\n",
    "        # Convert Heatmaps to Points By Taking The Maximum For Each Keypoint (Guesses)\n",
    "        G = []\n",
    "        for kp_ind in range(0,G_heatmaps.shape[0]):\n",
    "            Gm_ind = torch.argmax(G_heatmaps[kp_ind])\n",
    "            Gx_ind = Gm_ind.item() % G_heatmaps[kp_ind].shape[1]\n",
    "            Gy_ind = Gm_ind.item() // G_heatmaps[kp_ind].shape[1] # Indexed by top left corner (same as OpenCV)\n",
    "            G.append([Gx_ind, Gy_ind])\n",
    "        img_PIL = torchvision.transforms.ToPILImage()(I.type(torch.uint8))\n",
    "        img_cv = cv2.cvtColor(np.array(img_PIL), cv2.COLOR_GRAY2BGR)\n",
    "        for point_idx in range(0,NUM_POINTS):\n",
    "            cv2.circle(img_cv,((L[point_idx][0]),(L[point_idx][1])), KP_PLOT_RAD, (0,255,255))\n",
    "            cv2.circle(img_cv,((G[point_idx][0]),(G[point_idx][1])), KP_PLOT_RAD, (255,0,0))\n",
    "            #cv2.putText(img_cv,str(point_idx),(int(IMAGE_WIDTH*L[point_idx,0].item()),int(IMAGE_HEIGHT* (1 - L[point_idx,1].item()))),0,FONT_SCALE_PLOT,(0,255,0))\n",
    "            cv2.line(img_cv, ((L[point_idx][0]),(L[point_idx][1])),\\\n",
    "                     ((G[point_idx][0]),(G[point_idx][1])), (255,255,0))\n",
    "        plt.imshow(img_cv)\n",
    "        plt.show()\n",
    "        plt.imshow(L_heatmaps[0].numpy())\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.imshow(G_heatmaps[0].detach().numpy())\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "Initialize model and move to the main device (first GPU). Allow multi-GPU processing, and generate a loss function (moved to the main device for safety). Create an optimizer (must be created after moving model to GPU), then generate a data loader and traverse it in a training loop. A data loader should also be created for a validation set. Model weights are saved to the SSD when average validation set error reaches a new low. The entire block of code is wrapped in an if statement that protects multithreading on a Windows 10 OS.\n",
    "\n",
    "At the end of each forward pass of a batch, the most recent training batch loss is printed. At the end of each epoch, the average loss over the validation set is calculated, and the model's learnable parameters are saved to the SSD if this average loss is the new minimum over all previous average validation losses. If we are saving a new minimum, a sample of validation images with corresponding predicted segmented masks and an overlay of the two will be printed. The epoch training time, current index, average validation loss, and the current minimum average validation loss (along with the corresponding epoch index) are printed regardless of whether or not the average validation loss for the epoch was a new minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    # CUDA for PyTorch\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    main_device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True # Eventually makes faster after initial computational cost\n",
    "    \n",
    "    # Make model and move to main_device\n",
    "    model = PoseHighResolutionNet(num_key_points = NUM_POINTS, num_image_channels = IMG_CHANNELS)\n",
    "    model = model.to(main_device)\n",
    "\n",
    "    # Split Across Multiple Devices\n",
    "    if torch.cuda.device_count() > 1 and main_device.type == 'cuda':\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model) # Splits batches across GPUs\n",
    "        # Name additional devices\n",
    "        additional_devices = []\n",
    "        for device_id in range(1,torch.cuda.device_count()):\n",
    "            additional_devices.append(torch.device(\"cuda:\" + str(device_id)))\n",
    " \n",
    "    # Print info when using cuda\n",
    "    if main_device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(main_device),main_device,'- Main Device')\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(main_device)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(main_device)/1024**3,1), 'GB')                              \n",
    "        for extra_dev in additional_devices:\n",
    "            print(torch.cuda.get_device_name(extra_dev),extra_dev)\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(extra_dev)/1024**3,1), 'GB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_cached(extra_dev)/1024**3,1), 'GB')                              \n",
    "    \n",
    "    # Load model weights if loading from previous checkpoint\n",
    "    if LOAD_FROM_CHECKPOINT:\n",
    "        checkpoint = torch.load('./'+ MODEL_NAME + '.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print('Checkpoint average validation loss:',checkpoint['validation_loss'])\n",
    "    else:\n",
    "        start_epoch = -1\n",
    "        \n",
    "    # Make loss function and move to device (will run the cuda loss function if input tensor is a cuda tensor, but just in case)\n",
    "    loss_fn = torch.nn.MSELoss().to(main_device)\n",
    "    \n",
    "    # Make Adam optimizer\n",
    "    # It is recommended to move a model to GPU before constructing an optimizer:\n",
    "    # (https://pytorch.org/docs/master/optim.html)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    if LOAD_FROM_CHECKPOINT:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Training Generators\n",
    "    training_set = FeaturePointDataset(data_constants, STORE_DATA_RAM, 'training', NUM_POINTS)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **data_loader_parameters)\n",
    "    \n",
    "     # Validation Generators\n",
    "    validation_set = FeaturePointDataset(data_constants, STORE_DATA_RAM, 'validation', NUM_POINTS)\n",
    "    validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=VALIDATION_BATCH_SIZE, shuffle = True)\n",
    "    \n",
    "    # Initialize current minimums\n",
    "    if LOAD_FROM_CHECKPOINT:\n",
    "        current_minimum_avg_validation_loss = checkpoint['validation_loss']\n",
    "    else:\n",
    "        current_minimum_avg_validation_loss = math.inf\n",
    "    current_minimum_epoch_idx = 0\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch_idx in range(start_epoch + 1, MAX_EPOCHS):\n",
    "        # Epoch timer\n",
    "        epoch_timer_start = time.time()\n",
    "        \n",
    "        # Training loop for an epoch\n",
    "        print(\"-\"*65)\n",
    "        print('BEGIN EPOCH', epoch_idx)\n",
    "        model.train()\n",
    "        for training_batch_idx, training_batch in enumerate(training_generator):\n",
    "            # Transfer to GPU\n",
    "            training_batch, training_batch_labels = training_batch['image'].to(main_device, dtype=torch.float, non_blocking=True)\\\n",
    "            ,training_batch['label'].to(main_device, dtype=torch.float, non_blocking=True)\n",
    "            \n",
    "            # Zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward through the model\n",
    "            training_output = model(training_batch) # NOT NORMALIZING GRAYSCALE\n",
    "            \n",
    "            # Calculate loss\n",
    "            training_loss = loss_fn(training_output, training_batch_labels)\n",
    "            print('Epoch', epoch_idx, '--- Batch', training_batch_idx, 'training loss before optimization step:',\\\n",
    "                  training_loss.item())\n",
    "                \n",
    "            # Backprop then update using optimizer\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate average loss over validation set\n",
    "        total_validation_loss = 0\n",
    "        model.eval()\n",
    "        for validation_batch in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            validation_batch, validation_batch_labels = validation_batch['image'].\\\n",
    "            to(main_device, dtype=torch.float, non_blocking=True),\\\n",
    "            validation_batch['label'].to(main_device, dtype=torch.float, non_blocking=True)\n",
    "            \n",
    "            # Forward through the model\n",
    "            validation_output = model(validation_batch) # NOT NORMALIZING GRAYSCALE\n",
    "            \n",
    "            # Calculate loss and keep a running sum total of the loss over all the validation data\n",
    "            validation_loss = loss_fn(validation_output, validation_batch_labels)\n",
    "            total_validation_loss += validation_loss.item()*len(validation_batch)\n",
    "        average_validation_loss = total_validation_loss/len(validation_set)\n",
    "        '''\n",
    "        If the average validation loss is less than the current minimum, update the current minimum.\n",
    "        Also, print out original image/predicted segmentation image/overlay of pairs for a few samples from the validation set.\n",
    "        Finally, save the model's learned parameters to the SSD.\n",
    "        '''\n",
    "        if average_validation_loss < current_minimum_avg_validation_loss:\n",
    "            current_minimum_avg_validation_loss = average_validation_loss\n",
    "            current_minimum_epoch_idx = epoch_idx\n",
    "            print ('New minimum average validation loss found: ', current_minimum_avg_validation_loss)\n",
    "            \n",
    "            # Print out image, predicted segmentation mask, and an overlay\n",
    "            plot_predictions(validation_batch,validation_batch_labels, validation_output,NUM_PRINT_IMG)\n",
    "            \n",
    "            # Save the model\n",
    "            torch.save({'epoch': epoch_idx,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()\\\n",
    "                        ,'validation_loss': current_minimum_avg_validation_loss}, './'+ MODEL_NAME + '.pth')\n",
    "        elif LOAD_FROM_CHECKPOINT and epoch_idx == (start_epoch + 1): # Print out on first epoch if loading from a checkpoint\n",
    "            # Print out image, predicted segmentation mask, and an overlay\n",
    "            plot_predictions(validation_batch, validation_batch_labels, validation_output,NUM_PRINT_IMG)\n",
    "            current_minimum_epoch_idx = start_epoch\n",
    "\n",
    "        print('Epoch', epoch_idx, 'has average validation loss:', average_validation_loss)\n",
    "        print('Current minimum average validation loss:', current_minimum_avg_validation_loss,\\\n",
    "              '(from epoch ' + str(current_minimum_epoch_idx) + ')')\n",
    "        print ('Epoch', epoch_idx, 'runtime:',time.time() - epoch_timer_start,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test, Solve PNP, and Produce Kinematics/Evaluations\n",
    "Predicts 2D key points for an image from the test dataset, then uses OpenCV to solve the PNP problem and determine a rotation/translation 6 DOF pose for the model. Once this is done for all images, comparisons with the true kinematics are performed. Other tests on the accuracy of each point, different PnP solutions over other subsets of the key points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "The subection below loads all necessary test data into place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    # CUDA for PyTorch\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    main_device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True # Eventually makes faster after initial computational cost\n",
    "    \n",
    "    # Make model and move to main_device\n",
    "    model = PoseHighResolutionNet(num_key_points = NUM_POINTS, num_image_channels = IMG_CHANNELS)\n",
    "    model = model.to(main_device)\n",
    "\n",
    "    # Split Across Multiple Devices\n",
    "    if torch.cuda.device_count() > 1 and main_device.type == 'cuda':\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        model = nn.DataParallel(model) # Splits batches across GPUs\n",
    "        # Name additional devices\n",
    "        additional_devices = []\n",
    "        for device_id in range(1,torch.cuda.device_count()):\n",
    "            additional_devices.append(torch.device(\"cuda:\" + str(device_id)))\n",
    " \n",
    "    # Print info when using cuda\n",
    "    if main_device.type == 'cuda':\n",
    "        print(torch.cuda.get_device_name(main_device),main_device,'- Main Device')\n",
    "        print('Memory Usage:')\n",
    "        print('Allocated:', round(torch.cuda.memory_allocated(main_device)/1024**3,1), 'GB')\n",
    "        print('Cached:   ', round(torch.cuda.memory_cached(main_device)/1024**3,1), 'GB')                              \n",
    "        for extra_dev in additional_devices:\n",
    "            print(torch.cuda.get_device_name(extra_dev),extra_dev)\n",
    "            print('Memory Usage:')\n",
    "            print('Allocated:', round(torch.cuda.memory_allocated(extra_dev)/1024**3,1), 'GB')\n",
    "            print('Cached:   ', round(torch.cuda.memory_cached(extra_dev)/1024**3,1), 'GB')                              \n",
    "    \n",
    "    # Load model weights if loading from previous checkpoint\n",
    "    if LOAD_FROM_CHECKPOINT:\n",
    "        checkpoint = torch.load('./'+ MODEL_NAME + '.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print('Checkpoint average validation loss:',checkpoint['validation_loss'])\n",
    "    else:\n",
    "         raise Exception('Must be loading trained model if running in test mode!')\n",
    "            \n",
    "     # Make loss function and move to device (will run the cuda loss function if input tensor is a cuda tensor, but just in case)\n",
    "    loss_fn = torch.nn.MSELoss().to(main_device)\n",
    "        \n",
    "     # Test Generators\n",
    "    test_set = FeaturePointDataset(data_constants, STORE_DATA_RAM, 'test', NUM_POINTS)\n",
    "    test_generator = torch.utils.data.DataLoader(test_set, batch_size=VALIDATION_BATCH_SIZE, shuffle = False)\n",
    "    \n",
    "    '''\n",
    "    BEGIN READ IN IMAGE INFORMATION:\n",
    "    Width, Height, True Pose [X,Y,Z,ZA,XA,YA], Calibration File Info [PD,XO,YO,PP], \n",
    "    Model Key Points [NUM_POINTS by [X,Y,Z]], Projected Key Points [NUM_POINTS by [X,Y]]\n",
    "    \n",
    "    # Read In Each Line of Info File\n",
    "    info_file = TEST_OUTPUT_DIR + '/info_' + MODEL_TYPE +'.txt'\n",
    "    info_lines = [line.rstrip('\\n') for line in open(info_file)]\n",
    "    # Double Check That There Are 13*# of Test Files)\n",
    "    if len(info_lines) != 13*len(test_set):\n",
    "        raise Exception('Info file length is not 13 x the number of loaded test files!')\n",
    "\n",
    "    # Read in Projected Key Points File\n",
    "    # Projected Key Points\n",
    "    proj_kp_file = TEST_OUTPUT_DIR + '/' + MODEL_TYPE + '_KPlabels.txt'\n",
    "    proj_kp_lines = [line.rstrip('\\n') for line in open(proj_kp_file)]\n",
    "    # Double Check That There Are NUM_POINTS + 1*# of Test Files)\n",
    "    if len(proj_kp_lines) != (NUM_POINTS + 1)*len(test_set):\n",
    "        raise Exception('KP file length is not (NUM_POINTS + 1) x the number of loaded test files!')\n",
    "\n",
    "    # For Each Image, Create Dictionary with The Image's:\n",
    "    # Width, Height, True Pose [X,Y,Z,ZA,XA,YA], Calibration File Info [PD,XO,YO,PP], \n",
    "    # Model Key Points [NUM_POINTS by [X,Y,Z]], Projected Key Points [NUM_POINTS by [X,Y]]\n",
    "    filtered_image_info = []\n",
    "    print('Test set has', len(test_set),'images!')\n",
    "    for image_indx in range(len(test_set)):\n",
    "        # Image Width and Height\n",
    "        img_w = int(info_lines[image_indx*13 + 9].split(':')[1].strip())\n",
    "        img_h = int(info_lines[image_indx*13 + 10].split(':')[1].strip())\n",
    "        # True Pose\n",
    "        true_pose_str = info_lines[image_indx*13 + 11].split(':')[1].strip().split(',')\n",
    "        true_pose = [float(i) for i in true_pose_str]\n",
    "        # Get Image Home Directory\n",
    "        img_home_dir = os.path.dirname(info_lines[image_indx*13].split('Path:')[1].strip())\n",
    "        # Calibration Info\n",
    "        calib_info_str = [line.strip() for line in open(img_home_dir + '/calibration.txt')][1:5]\n",
    "        calib_info = [float(i) for i in calib_info_str]\n",
    "        # Model Key Points (NUM_POINTS by (X,Y,Z))\n",
    "        model_kp_whole_file = [line.rstrip('\\n') for line in open(img_home_dir + '/'+ MODEL_TYPE +'.kp')]\n",
    "        # Check there are 2 + NUM_POINTS LINES\n",
    "        if len(model_kp_whole_file) != 2 + NUM_POINTS:\n",
    "            raise Exception('Model key points file is not the right length!')\n",
    "        model_kp_str = [line.split(':')[1].strip().split(',') for line in model_kp_whole_file[1:NUM_POINTS+1]]\n",
    "        model_kp = [[float(j) for j in i] for i in model_kp_str]\n",
    "        # Projected Key Points\n",
    "        proj_kp_str = [line.strip().split(',') for line in proj_kp_lines[image_indx*(NUM_POINTS+1) + 1:\\\n",
    "                                                                          (image_indx*(NUM_POINTS+1) + NUM_POINTS + 1)]]\n",
    "        proj_kp = [[float(j) for j in i] for i in proj_kp_str]\n",
    "        # Store in Dictionary\n",
    "        filtered_image_info.append({'image_width': img_w, 'image_height': img_h,'true_pose': true_pose,'calibration': calib_info,\\\n",
    "                                 'model_kp': model_kp, 'projected_kp': proj_kp})\n",
    "    \n",
    "    \n",
    "    END READ IN IMAGE INFORMATION\n",
    "    '''\n",
    "    # Loop: predict, print and save\n",
    "    model.eval()\n",
    "    images_saved = 0\n",
    "    total_test_loss = 0\n",
    "    output_storage = np.empty([len(test_set), NUM_POINTS, 2])\n",
    "    test_index_counter = 0\n",
    "    for test_batch in test_generator:\n",
    "        # Transfer to GPU\n",
    "        test_batch, test_batch_labels = test_batch['image'].\\\n",
    "        to(main_device, dtype=torch.float, non_blocking=True),\\\n",
    "        test_batch['label'].to(main_device, dtype=torch.float, non_blocking=True)\n",
    "\n",
    "        # Forward through the model\n",
    "        test_output = model(test_batch)\n",
    "        \n",
    "        # Calculate loss and keep a running sum total of the loss over all the test data\n",
    "        test_loss = loss_fn(test_output, test_batch_labels)\n",
    "        total_test_loss += test_loss.item()*len(test_batch)\n",
    "        \n",
    "        # Print out image, predicted segmentation mask, and an overlay\n",
    "        plot_predictions(test_batch,test_batch_labels, test_output,VALIDATION_BATCH_SIZE)\n",
    "\n",
    "        # Store Outputs\n",
    "        # Convert Heatmaps to Points By Taking The Maximum For Each Keypoint (Labels)\n",
    "        heatmaps_output = test_output.cpu().detach()\n",
    "        for tb_ind in range(len(test_batch)):\n",
    "            KP_loc_guess = []\n",
    "            for kp_ind in range(NUM_POINTS):\n",
    "                m_ind = torch.argmax(heatmaps_output[tb_ind][kp_ind])\n",
    "                x_ind = m_ind.item() % heatmaps_output[tb_ind][kp_ind].shape[1]\n",
    "                y_ind = m_ind.item() // heatmaps_output[tb_ind][kp_ind].shape[1] # Indexed by top left corner (same as OpenCV)\n",
    "                KP_loc_guess.append([x_ind/IMAGE_WIDTH, (IMAGE_HEIGHT - y_ind)/IMAGE_HEIGHT])\n",
    "            output_storage[test_index_counter+tb_ind,:,:] = KP_loc_guess\n",
    "         \n",
    "        test_index_counter += len(test_batch)\n",
    "    # Print out average test batch loss\n",
    "    average_test_loss = total_test_loss/len(test_set)\n",
    "    print('Average loss over test set:',average_test_loss)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Predicted Points Output\n",
    "Writes the 2D predicted points for each test image to a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must Create New Output File\n",
    "file = open(DATA_HOME_DIR + \"/\" + \"predicted_points.txt\", \"w\")\n",
    "for data_idx, data_image in enumerate(output_storage):\n",
    "    file.write(\"Image_\" + str(data_idx) + \"\\n\")\n",
    "    for out_point in data_image:\n",
    "        file.write(str(out_point[0]) + \",\" + str(out_point[1]) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinematics Predictor\n",
    "Predicts kinematics using PNP solvers on various subsets of the data predicted from the neural network. For each image in the test set, a pose is guessed and displayed. The actual true pose, the absolute error, and the running average of the absolute error are also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Each Frame, Run Metrics Like Which Points Are The Most Accurate\n",
    "kinematics_storage = np.empty([len(test_set), 6]) # [T_X, T_Y, T_Z, R_Z, R_X, R_Y]\n",
    "point_guess_L2_Error_storage = [] # L2 Error stored for each of the NUM_Points\n",
    "for num_points in range(NUM_POINTS):\n",
    "    point_guess_L2_Error_storage.append([])\n",
    "kinematic_L1_error_storage = [] # L1 Error stored for each of the kinematics poses\n",
    "for pose_dof in range(6):\n",
    "    kinematic_L1_error_storage.append([])\n",
    "kp_used_storage = [] # Stores each time a key point is used in an ideal kinematic\n",
    "for test_image_idx, test_image in enumerate(test_set):\n",
    "    \n",
    "    #Print out Test Image For Viewing\n",
    "    #plt.figure(figsize=(16,(IMAGE_HEIGHT/IMAGE_WIDTH)*16))\n",
    "    #I = test_image['image'].to(\"cpu\").type(torch.float32)\n",
    "    #img_PIL = torchvision.transforms.ToPILImage()(I.type(torch.uint8))\n",
    "    #plt.imshow(img_PIL)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Convert Guesses for Each Point in Image to Tuples That Have been Unnormalized to\n",
    "    #  ORIGINAL Image Dimensions (Not Necessarily Training Image Dimensions) Which Have Been Read In\n",
    "    original_image_width = filtered_image_info[test_image_idx]['image_width']\n",
    "    original_image_height = filtered_image_info[test_image_idx]['image_height']\n",
    "    image_points_guesses = np.array([tuple([output_storage[test_image_idx,point,0] * original_image_width,\\\n",
    "                           output_storage[test_image_idx,point,1] * original_image_height]) for point in range(NUM_POINTS)])\n",
    "    # Read in Calibration File for Image\n",
    "    principal_distance = filtered_image_info[test_image_idx]['calibration'][0]\n",
    "    principal_x = filtered_image_info[test_image_idx]['calibration'][1]\n",
    "    principal_y = filtered_image_info[test_image_idx]['calibration'][2]\n",
    "    pixel_pitch = filtered_image_info[test_image_idx]['calibration'][3]\n",
    "\n",
    "    # Add L2 Point Errors\n",
    "    for point_indx in range(NUM_POINTS):\n",
    "        point_guess_L2_Error_storage[point_indx].append(math.sqrt(\\\n",
    "                  (pixel_pitch*(image_points_guesses[point_indx][0] -\\\n",
    "                                filtered_image_info[test_image_idx]['projected_kp'][point_indx][0]* original_image_width))**2 +\\\n",
    "                  (pixel_pitch*(image_points_guesses[point_indx][1] -\\\n",
    "                                filtered_image_info[test_image_idx]['projected_kp'][point_indx][1]* original_image_height))**2))\n",
    "\n",
    "    # Read in Correct Location(s) of Points for Image\n",
    "    model_points = np.array([tuple(mod_kp) for mod_kp in filtered_image_info[test_image_idx]['model_kp']], dtype=\"double\")\n",
    "\n",
    "    # Create Camera Matrix For Image\n",
    "    camera_matrix = np.array([\n",
    "                                [-1.0*principal_distance/pixel_pitch, 0, original_image_width/2.0 - principal_x/pixel_pitch],\n",
    "                                [0, -1.0*principal_distance/pixel_pitch, original_image_height/2.0 - principal_y/pixel_pitch],\n",
    "                                [0, 0, 1]\n",
    "                            ], dtype = \"double\")\n",
    "\n",
    "    # Solve PNP for All Combinations of Size NUM_GUESSES_FOR_PNP from the image_points_guesses\n",
    "    # The one with the least reprojection error will be taken as the estimate pose\n",
    "    NUM_GUESSES_FOR_PNP = NUM_POINTS   \n",
    "    dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "    min_reproj_err_val = math.inf\n",
    "    arg_min_val_indices = []\n",
    "    for comb in itertools.combinations(range(NUM_POINTS), NUM_GUESSES_FOR_PNP):\n",
    "        # Solve PNP\n",
    "        (success, rotation_vector, translation_vector, inliers) = cv2.solvePnPRansac(\\\n",
    "                                                                      model_points[list(comb)],\\\n",
    "                                                                      image_points_guesses[list(comb)],\\\n",
    "                                                                      camera_matrix, dist_coeffs,\\\n",
    "                                                                      reprojectionError=20.0)\n",
    "\n",
    "        # Get Reprojection Error\n",
    "        projected_PNP_guesses, _ = cv2.projectPoints(\\\n",
    "                                                     model_points[list(comb)],\\\n",
    "                                                     rotation_vector,\\\n",
    "                                                     translation_vector,\\\n",
    "                                                     camera_matrix, dist_coeffs)\n",
    "        projected_PNP_guesses = np.squeeze(projected_PNP_guesses)\n",
    "        reproj_error = ((projected_PNP_guesses - image_points_guesses[list(comb)])**2).mean()\n",
    "\n",
    "        # Convert to RzRxRy Euler Angles\n",
    "        R = cv2.Rodrigues(rotation_vector)[0]\n",
    "        if (np.asscalar(translation_vector[2]) > 0): # Flip to correct side of camera by multiplying R and T by -1\n",
    "            translation_vector = -1*translation_vector\n",
    "            R[0:2,:] =-1*R[0:2,:]\n",
    "        theta_x = math.asin(R[2,1])\n",
    "        if (theta_x < math.pi/2):\n",
    "            if (theta_x > -1*math.pi/2):\n",
    "                theta_z = math.atan2(-1*R[0,1],R[1,1])\n",
    "                theta_y = math.atan2(-1*R[2,0],R[2,2])\n",
    "            else:\n",
    "                # Not a unique solution\n",
    "                theta_z = -1*math.atan2(R[0,2],R[0,0])\n",
    "                theta_y = 0\n",
    "        else:\n",
    "            # Not a unique solution\n",
    "            theta_z = math.atan2(R[0,2],R[0,0])\n",
    "            theta_y = 0\n",
    "\n",
    "        if (reproj_error < min_reproj_err_val):\n",
    "            min_reproj_err_val = reproj_error\n",
    "            rot_z = 360*theta_z/(2*math.pi)\n",
    "            if rot_z < -180:\n",
    "                rot_z += 360\n",
    "            if rot_z > 180:\n",
    "                rot_z -= 360\n",
    "            rot_x = 360*theta_x/(2*math.pi)\n",
    "            if rot_x < -180:\n",
    "                rot_x += 360\n",
    "            if rot_x > 180:\n",
    "                rot_x -= 360\n",
    "            rot_y = 360*theta_y/(2*math.pi)\n",
    "            if rot_y < -180:\n",
    "                rot_y += 360\n",
    "            if rot_y > 180:\n",
    "                rot_y -= 360\n",
    "            kinematic_guess_for_image = [ np.asscalar(translation_vector[0]), np.asscalar(translation_vector[1]), np.asscalar(translation_vector[2]),\n",
    "                                                rot_z,\n",
    "                                                rot_x,\n",
    "                                                rot_y]\n",
    "            arg_min_val_indices = tuple(indices[0] for indices in inliers)\n",
    "    print(kinematic_guess_for_image)\n",
    "    print(filtered_image_info[test_image_idx]['true_pose'])\n",
    "    for pose_dof in range(6):\n",
    "        L1_abs_err = abs(filtered_image_info[test_image_idx]['true_pose'][pose_dof] \\\n",
    "                                                       - kinematic_guess_for_image[pose_dof])\n",
    "        if pose_dof > 2: # IF ROTATION\n",
    "            if L1_abs_err > 180:\n",
    "                L1_abs_err = 360 - L1_abs_err\n",
    "        kinematic_L1_error_storage[pose_dof].append(L1_abs_err)\n",
    "    print(np.mean(kinematic_L1_error_storage, axis=1))\n",
    "    kp_used_storage.append(arg_min_val_indices)\n",
    "    print(\"--\"*20)\n",
    "    print(\"--\"*20)\n",
    "\n",
    "    # Store Kinematics\n",
    "    kinematics_storage[test_image_idx,:] = kinematic_guess_for_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram KP Error\n",
    "Histograms of key point error in-plane for each of the NUM_POINTS key points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms of Each Key Point Error\n",
    "for data_idx, data in enumerate(point_guess_L2_Error_storage):\n",
    "    # fixed bin size\n",
    "    bins = np.arange(0, 100, 1) # fixed bin size\n",
    "    plt.xlim([min(data)-5, max(data)+5])\n",
    "    #plt.xlim([0, 20])\n",
    "    plt.hist(data, bins=bins, alpha=0.5)\n",
    "    plt.title('Histogram of L2 Error (mm) for Point %i' %(data_idx+1))\n",
    "    plt.xlabel('L2 Error (bin size = 1)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram DOF Error\n",
    "Histograms that plot the absolute error in mm/deg for each of the degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histograms of DOF Error\n",
    "for data_idx, data in enumerate(kinematic_L1_error_storage):\n",
    "    # fixed bin size\n",
    "    bins = np.arange(0, 1000, 1) # fixed bin size\n",
    "    plt.xlim([min(data)-5, max(data)+5])\n",
    "    #plt.xlim([0, 20])\n",
    "    plt.hist(data, bins=bins, alpha=0.5)\n",
    "    plt.title('Histogram of Absolute Error Chart %i' %(data_idx+1))\n",
    "    plt.xlabel('Absolute Error (bin size = 1)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of Number of Times a KP Used in Guess for Ideal Pose\n",
    "Histogram of the number of times each key point (indexed from zero) was used in the guess for the true pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(itertools.chain(*kp_used_storage))\n",
    "# fixed bin size\n",
    "bins = np.arange(0, 1000, 1) # fixed bin size\n",
    "plt.xlim([min(data), max(data) + 1])\n",
    "plt.hist(data, bins=bins, alpha=0.5)\n",
    "plt.title('Histogram of Number of Times a Key Point Was Used in an Ideal Kinematic')\n",
    "plt.xlabel('Key Point Index')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Kinematics File\n",
    "Writes an estimated kinematics file (.jtak) for each study and places it in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Info Lines To Get Image Paths\n",
    "image_paths_from_info_file = info_lines[0::13]\n",
    "\n",
    "# Sort Alphabetically and Get Indices\n",
    "sorted_image_paths = np.sort(image_paths_from_info_file)\n",
    "arg_sorted_image_paths = np.argsort(image_paths_from_info_file)\n",
    "\n",
    "# Parse Sorted Image Paths to Get Study Directories\n",
    "sorted_directories = [os.path.dirname(info[12:]) for info in sorted_image_paths]\n",
    "\n",
    "# Create and Fill Kinematics Estimate Files\n",
    "prev_dir = \"\"\n",
    "SIG_FIGS = 4\n",
    "for indx_sd, test_img in enumerate(sorted_directories):\n",
    "    if  prev_dir != test_img:\n",
    "        if prev_dir != \"\":\n",
    "            file.close()\n",
    "        prev_dir = test_img\n",
    "        # Must Create New Kinematics File\n",
    "        file = open(test_img + \"/\" + MODEL_TYPE + \"_estimated_kin_\" + MODEL_NAME + \".jtak\", \"w\")\n",
    "        file.write(\"JTA_EULER_KINEMATICS\\nX_TRAN\\t\\tY_TRAN\\t\\tZ_TRAN\\t\\tZ_ROT\\t\\tX_ROT\\t\\tY_ROT\\n\")\n",
    "    est_kin = kinematics_storage[arg_sorted_image_paths[indx_sd]]\n",
    "    file.write(str(round(est_kin[0],4)) + \",\\t\\t\" + str(round(est_kin[1],4)) + \",\\t\\t\" + str(round(est_kin[2],4))\\\n",
    "               + \",\\t\\t\" + str(round(est_kin[3],4)) + \",\\t\\t\"  + str(round(est_kin[4],4)) + \",\\t\\t\" + str(round(est_kin[5],4)) + \",\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Kinematics Files\n",
    "Deletes estimated kinematics files from folders once finished (so as to not interfere with the Study to Grid writer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes Kinematics Estimate Files\n",
    "prev_dir = \"\"\n",
    "for indx_sd, test_img in enumerate(sorted_directories):\n",
    "    if  prev_dir != test_img:\n",
    "        if prev_dir != \"\":\n",
    "            file.close()\n",
    "        prev_dir = test_img\n",
    "        # Must Create New Kinematics File\n",
    "        os.remove(test_img + \"/\" + MODEL_TYPE + \"_estimated_kin_\" + MODEL_NAME + \".jtak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
